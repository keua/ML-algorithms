{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we will study 2 different Machine Learning algorithms **Perceptron Learning Algorithm (PLA)** and its variant **ADaptative LInear NEuron (ADALINE)**. We will do that through doing a set of experiments for each algorithm and analyzing its results to finally discuss the main features of each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning from data\n",
    "\n",
    "As it is mentioned earlier, we will study machine learning (ML) algorithms but for that, we should have an idea of what ML is about. The book **Learning from data** compares ML with how we actually learn, for example we learn about flowers looking at them and not studying their mathematical definition. We can say in other words, that we learn from data. This is important to understand since what we are trying to do in ML is exactly that, get data, train our algorithm, test it and then evaluate if it is performing well. The book use the following diagrama to illustrate a basic setup of the learning problem.\n",
    "\n",
    "<img src=\"images/lfd.png\" alt=\"drawing\" width=\"400\"/>\n",
    "**Reference:** Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin. “Learning From Data” AMLBook.\n",
    "\n",
    "The diagram above illustrate very well the components of a learning problem. We have basically a unknown function $f$ which is in fact the target function we want to learn. To learn the target function $f$ we need a data set conformed by a set of pairs $(x_1,y_y),(x_2,y_n),\\dots,(x_n,y_n)$ that represents the input values of a function with its respective value after evaluation. We us this set to help the algorithm classifying between different classes of data. The learning algorithm is in charge of the classification problem which is also related with a hypothesis. The hypothesis set consits in a set of candidates functions to learn $f$. Finally we end up with a final hypothesis $g$ which is the one that better approsimates $f$.\n",
    "\n",
    "So in this work we are focused in the implementation of **learning algorithms** that will help us to find a good final hypothesis $g$ to classify our data in the right way. We will now present the description of the implemented algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Learning Algorithm\n",
    "\n",
    "The perceptron learning algorithm belongs to the class of linear classifiers, this is, for a data set classified according to binary categories labeled +1 and -1, the classifiers goal is to divide the two classes by a linear separator. The separator is a $(n-1)-dimensional$ hyperplane in a $n-dimensional$ space, For instance in 3-dimensional space, the classifier is a line.\n",
    "\n",
    "The data set will be assumed to consist of $N$ observations characterized by $d$ features or attributes, $ \\mathbf{x}_n = (x_1, \\ldots, x_d)$ for $n = (1, \\ldots, N)$. The binary classification problem can be translated to that of finding a series of weights  $w_i$ such that all vectors verifying\n",
    "\n",
    "<center>$\\displaystyle \\sum_{i=1}^d w_i x_i < b$</center>\n",
    "\n",
    "are assigned to one of the classes whereas those verifying\n",
    "\n",
    "<center>$\\displaystyle \\sum_{i=1}^d w_i x_i > b$</center>\n",
    "\n",
    "are assigned to the other, for a given threshold value b. If we rename $b = w_0$ and introduce an artificial coordinate $x_0 = 1$ in our vectors $\\mathbf{x}_n$, we can write the perceptron separator formula as\n",
    "\n",
    "<center>\n",
    "$\\displaystyle h(\\mathbf{x}) = \\mathrm{sign}\\left(\\sum_{i=0}^d w_i x_i\\right) = \\mathrm{sign}\\left( \\mathbf{w}^{\\mathbf{T}}\\mathbf{x}\\right)$\n",
    "</center>\n",
    "\n",
    "$\\mathbf{w}^{\\mathbf{T}}\\mathbf{x}$ is notation for the scalar product between vectors $\\mathbf{w}$ and $\\mathbf{x}$. Therefore the problem of classifying is that of finding the vector of weights $\\mathbf{w}$ given a training data set of $N$ vectors $\\mathbf{x}$ with their corresponding labeled classification vector $(y_1, \\ldots, y_N)$.\n",
    "\n",
    "The algorithm starts with a guess for the vector $\\mathbf{w}$. It then verify how good the guess is by comparing the predicted labels with the actual, correct labels. When misclassified points remain, the algorithm corrects its guess for the weight vector by updating the weights in the right direction, until all points are correctly classified.\n",
    "\n",
    "An important feature to mention aboutthe perceptron learning rule is that if there exist a set  of linearly separable data, then the perceptron will always find these weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptative Linear Neuron\n",
    "\n",
    "Adaline can be viewed as a variation of PLA with update scaled by the mismatch magnitude. While not the universally best choice for the classifier learning algorithm, Adaline can be viewed as an improvement over the original PLA.\n",
    "The Adaline classifier is closely related to the Ordinary Least Squares (OLS) Linear Regression algorithm; in OLS regression we find the line (or hyperplane) that minimizes the vertical offsets. Or in other words, we define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples i in our dataset of size n.\n",
    "\n",
    "<center> $SSE=\\sum_{i}(target^{(i)}−output^{(i)})^{2}$ </center>\n",
    "<br>\n",
    "<center>$MSE=\\frac{1}{n}×SSE$</center>\n",
    "\n",
    "LinearRegression implements a linear regression model for performing ordinary least squares regression, and in Adaline, we add a threshold function $g(⋅)$ to convert the continuous outcome to a categorical class label:\n",
    "<center> $ y = g({z}) = \\begin{cases}1 & z \\geq 0\\\\-1 & otherwise\\end{cases} $</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision\n",
    "\n",
    "The Perceptron uses the class labels to learn model coefficients, On the other hand Adaline uses continuous predicted values to learn the model coefficients, which is more powerful since it tells us by how much we were right or wrong. So, in the perceptron, as illustrated below, we simply use the predicted class labels to update the weights, and in Adaline, we use a continuous response.\n",
    "\n",
    "![pla-adaline](images/pla-ada.png)\n",
    "**Reference**: [here](https://rasbt.github.io/mlxtend/user_guide/classifier/Adaline/).\n",
    "\n",
    "Both learning algorithms can actually be summarized by 4 simple steps \n",
    "\n",
    "- given that the stochastic gradient descent was used for Adaline:\n",
    "\n",
    "1. Initialize the weights to 0 or small random numbers.\n",
    "2. For each training sample:\n",
    "    1. Calculate the output value.\n",
    "    2. Update the weights.\n",
    "\n",
    "We write the weight update in each iteration as:\n",
    "\n",
    "<center> $w_{j}:=w_{j} + \\Delta w_{j}$ </center>\n",
    "\n",
    "where\n",
    "\n",
    "<center>$\\Delta w_{j}=\\eta (target^{(i)}−output^{(i)})x_{j}^{i} $</center>\n",
    "\n",
    "the **output** is the continuous net input value in Adaline and the predicted class label in case of the perceptron; $\\eta$ is the learning rate. The weight update in Adaline is basically just taking the **opposite step** in direction of the sum-of-squared error cost gradient, for more detail you can to [here](http://rasbt.github.io/mlxtend/user_guide/general_concepts/linear-gradient-derivative/).\n",
    "\n",
    "The algorithm also have similarities linke for example: \n",
    "- They are classifiers for binary classification\n",
    "- Both have a linear decision boundary\n",
    "- Both can learn iteratively, sample by sample (the Perceptron naturally, and Adaline via stochastic gradient descent)\n",
    "- Both use a threshold function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "Both algorithms have been implented using **Python 3.6.3**. We also used auxilary libraries like numpy, matplotlib among others that helped to draw graphs in order to better understand the results. There is basically a class called **Perceptron** in which both variants **PLA** and **ADALINE** were programmed. A function to generate linearly separable data was also programmed and it will be helpful for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Libraries needed to implement and visualize the PLA algorithm and its variant \n",
    "Adaline.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import rcParams\n",
    "import imageio\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Images path\n",
    "IMAGE_PATH = 'images/'\n",
    "\n",
    "# setting up the figure size\n",
    "rcParams[\"figure.figsize\"] = 5, 5\n",
    "rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    PLA, Perceptron Learning Algorithm and Adaline, Adaptive Linear Neuron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        N : Integer\n",
    "            Number of linearly separable points to generate.\n",
    "        datarseed : Integer\n",
    "            Seed used to initialize the datas random generator.\n",
    "            Default: 1324\n",
    "        rmispts : Boolean\n",
    "            If True the misclassified points are selected randomly.\n",
    "            Default: False\n",
    "        misptsrseed : Integer\n",
    "            Seed used to initialize the misclasifications random generator.\n",
    "            Default: 1324\n",
    "        dimension : Integer\n",
    "            Dimension in which the points are generated.\n",
    "            Default: 2\n",
    "    Attributes\n",
    "    ----------\n",
    "        w : array\n",
    "            Weights vector.\n",
    "        X : array, shape = [nsamples, nfeatures]\n",
    "            Training vectors, where 'nasamples' is the number of\n",
    "            samples and 'nfeatures' is the number of features.\n",
    "        V : array\n",
    "            Line in 2d or n-plane in n-dimensions that separates the points.\n",
    "        dim : array\n",
    "            Dimension in which the algorithm works.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, N, datarseed=1324, rmispts=False, misptsrseed=1234, dimension=2\n",
    "    ):\n",
    "\n",
    "        self.dim = dimension\n",
    "        self.rgendata = np.random.RandomState(datarseed)\n",
    "        self.rgenmispts = np.random.RandomState(misptsrseed)\n",
    "        self.rmispts = rmispts\n",
    "\n",
    "        # Random linearly separated data,\n",
    "        var = [self.rgendata.uniform(-1, 1) for i in range(self.dim * 2)]\n",
    "        for i in range(self.dim + 1):\n",
    "            if i == 0:\n",
    "                self.V = np.array(\n",
    "                    var[i + 2] * var[i + 1] - var[i] * var[i + 3])\n",
    "            elif i % 2 == 0:\n",
    "                self.V = np.append(self.V, var[i] - var[i - 1])\n",
    "            else:\n",
    "                self.V = np.append(self.V, var[i])\n",
    "\n",
    "        self.X = self.generate_points(N)\n",
    "\n",
    "    def generate_points(self, N):\n",
    "        \"\"\"\n",
    "        Generate linearly separable data points method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        N : Integer\n",
    "            Number of data points to generate.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        X : array, shape = [nsamples, nfeatures]\n",
    "            Training vectors, where 'nasamples' is the number of\n",
    "            samples and 'nfeatures' is the number of features.\n",
    "\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        for i in range(N):\n",
    "            xn = [self.rgendata.uniform(-1, 1) for i in range(self.dim)]\n",
    "            x = np.append([1], xn)\n",
    "            s = int(np.sign(self.V.T.dot(x)))\n",
    "            X.append((x, s))\n",
    "        return X\n",
    "\n",
    "    def plot(self, mispts=None, vec=None, save=False, imgname='tmp'):\n",
    "        \"\"\"\n",
    "        2D data plot method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        mispts : array\n",
    "            Set of misclassified data points.\n",
    "            Default: None\n",
    "        vec : array\n",
    "            Weights vector that describes the hypotesis.\n",
    "            Default: None\n",
    "        save : Boolean\n",
    "            If the value is True the plots are saved to disk.\n",
    "            Default: False\n",
    "        imgname : String\n",
    "            Name used to save the plot to disk.\n",
    "            Default: 'tmp'\n",
    "\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        plt.xlim(-1, 1)\n",
    "        plt.ylim(-1, 1)\n",
    "        V = self.V\n",
    "\n",
    "        a, b = -V[1] / V[2], -V[0] / V[2]\n",
    "        l = np.linspace(-1, 1)\n",
    "\n",
    "        plt.plot(l, a * l + b, 'k-')\n",
    "        cols = {1: 'r', -1: 'b'}\n",
    "\n",
    "        if mispts == None:\n",
    "            for x, s in self.X:\n",
    "                plt.plot(x[1], x[2], cols[s] + 'o')\n",
    "\n",
    "        if mispts:\n",
    "            for x, s in mispts:\n",
    "                plt.plot(x[1], x[2], cols[s] + '.')\n",
    "\n",
    "        if vec != None:\n",
    "            aa, bb = -vec[1] / vec[2], -vec[0] / vec[2]\n",
    "            plt.plot(l, aa * l + bb, 'g-', lw=2)\n",
    "\n",
    "        if save:\n",
    "            if not mispts:\n",
    "                plt.title('N = %s' % (str(len(self.X))))\n",
    "                plt.xlabel('x1')\n",
    "                plt.ylabel('x2')\n",
    "            else:\n",
    "                plt.title(\n",
    "                    'N = %s with %s test points' %\n",
    "                    (str(len(self.X)), str(len(mispts)))\n",
    "                )\n",
    "                plt.xlabel('x1')\n",
    "                plt.ylabel('x2')\n",
    "            plt.savefig(\n",
    "                IMAGE_PATH + imgname, dpi=50, bbox_inches='tight'\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "    def classification_error(self, vec, pts=None):\n",
    "        \"\"\"\n",
    "        Error classification method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        vec : array\n",
    "            Weights vector used to classify a point.\n",
    "        pts : array\n",
    "            Data points to evaluate:\n",
    "            Default: self.X\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        error : float\n",
    "            Fraction of misclassified points.\n",
    "        mispts: array\n",
    "            Set of misclassified points.\n",
    "\n",
    "        \"\"\"\n",
    "        # Error defined as fraction of misclassified points\n",
    "        if not pts:\n",
    "            pts = self.X\n",
    "        M = len(pts)\n",
    "        n_mispts = 0\n",
    "        mispts = []\n",
    "        for x, s in pts:\n",
    "            if int(np.sign(vec.T.dot(x))) != s:\n",
    "                n_mispts += 1\n",
    "                mispts.append((x, s))\n",
    "        error = n_mispts / float(M)\n",
    "        return error, mispts\n",
    "\n",
    "    def choose_miscl_point(self, vec):\n",
    "        \"\"\"\n",
    "        Choose a misclassified point method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        vec : array\n",
    "            Weights vector used to classify a point.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        mispts[i] : array\n",
    "            Selected misclassified data point.\n",
    "\n",
    "        \"\"\"\n",
    "        # Choose a random point among the misclassified\n",
    "        pts = self.X\n",
    "        mispts = []\n",
    "        for x, s in pts:\n",
    "            if int(np.sign(vec.T.dot(x))) != s:\n",
    "                if not self.rmispts:\n",
    "                    return (x, s)\n",
    "                mispts.append((x, s))\n",
    "        return mispts[self.rgenmispts.randint(0, len(mispts))]\n",
    "\n",
    "    def pla(self, save=False, suffix=''):\n",
    "        \"\"\"\n",
    "        Perceptron learning algorithm method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        save : Boolean\n",
    "            If value is True Save the sequence of plots in which the algorithm \n",
    "            go through.\n",
    "        suffix : String\n",
    "            suffix to add to the name used to save the sequence of plots to disk.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        it : Integer\n",
    "            Number of iterations the algorithm took to converge.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the weights to zeros\n",
    "        w = np.zeros(self.dim + 1)\n",
    "        X, N = self.X, len(self.X)\n",
    "\n",
    "        it = 0\n",
    "        # Iterate until all points are correctly classified\n",
    "        while self.classification_error(w)[0] != 0:\n",
    "            it += 1\n",
    "            # Pick random misclassified point\n",
    "            x, s = self.choose_miscl_point(w)\n",
    "            # Update weights\n",
    "            w += s * x\n",
    "            if save:\n",
    "                self.plot(vec=w.tolist())\n",
    "                plt.title('N = %s, Iteration %s\\n' % (str(N), str(it)))\n",
    "                plt.xlabel('x1')\n",
    "                plt.ylabel('x2')\n",
    "                plt.savefig(\n",
    "                    IMAGE_PATH + suffix + 'p_N%s_it%s' % (str(N), str(it)),\n",
    "                    dpi=50, bbox_inches='tight'\n",
    "                )\n",
    "                plt.close()\n",
    "        self.w = w\n",
    "        return it\n",
    "\n",
    "    def adaline(self, save=False, suffix='', lrate=1, limit=10):\n",
    "        \"\"\"\n",
    "        Adaptive linear neuron algorithm method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        save : Boolean\n",
    "            If value is True Save the sequence of plots in which the algorithm \n",
    "            go through.\n",
    "        suffix : String\n",
    "            suffix to add to the name used to save the sequence of plots to disk.\n",
    "        lrate : float\n",
    "            Learning rate factor.\n",
    "        limit : Integer\n",
    "            Maximum number of updates the algorithm does.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize the weigths to zeros\n",
    "        w = np.zeros(self.dim + 1)\n",
    "        X, N = self.X, len(self.X)\n",
    "\n",
    "        it = 0\n",
    "        # Iterate until all points are correctly classified\n",
    "        while it < limit and self.classification_error(w)[0] != 0:\n",
    "            it += 1\n",
    "            # Pick random misclassified point\n",
    "            x, s = self.choose_miscl_point(w)\n",
    "            y = w.T.dot(x)\n",
    "            # Update weights\n",
    "            w += lrate * (s - y) * x\n",
    "            if save:\n",
    "                self.plot(vec=w.tolist())\n",
    "                plt.title('N = %s, Iteration %s\\n' % (str(N), str(it)))\n",
    "                plt.xlabel('x1')\n",
    "                plt.ylabel('x2')\n",
    "                plt.savefig(\n",
    "                    IMAGE_PATH + suffix + 'p_N%s_it%s' % (str(N), str(it)),\n",
    "                    dpi=50, bbox_inches='tight'\n",
    "                )\n",
    "                plt.close()\n",
    "        self.w = w\n",
    "        return it\n",
    "\n",
    "    def check_error(self, M, vec):\n",
    "        \"\"\"\n",
    "        Test data error check method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        N : Integer\n",
    "            Number of data points to generate.\n",
    "        vec : array\n",
    "            Weights vector, generated by running the algorithm with a training\n",
    "            set, used to classify the data points.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        error : float\n",
    "            Fraction of misclassified points.\n",
    "        miscpoints: array\n",
    "            Set of misclassified points.\n",
    "        check_pts: array\n",
    "            The generated data points.\n",
    "\n",
    "        \"\"\"\n",
    "        check_pts = self.generate_points(M)\n",
    "        error, miscpoints = self.classification_error(vec, pts=check_pts)\n",
    "        return error, miscpoints, check_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "\n",
    "As part of the implemenation, a visualization functions were programmed. These functions allow us to show the graphs and results in such a way to clearly see the results. We also used some HTML code and generate gif animations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_images(suffix):\n",
    "    \"\"\"\n",
    "    Get the list of images associated to a suffix.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    suffix : String\n",
    "        Suffix of set of requested images.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    sortpngs : array\n",
    "        Set of images (plots) names.\n",
    "\n",
    "    \"\"\"\n",
    "    basedir = os.getcwd()\n",
    "    os.chdir(basedir)\n",
    "    pngs = [pl for pl in os.listdir(IMAGE_PATH) if pl.endswith(\n",
    "        'png') and pl.startswith(suffix)]\n",
    "    sortpngs = sorted(pngs, key=lambda a: int(a.split('it')[1][:-4]))\n",
    "    return sortpngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_animation(image1, image2):\n",
    "    \"\"\"\n",
    "    Show images or animations method.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    image1 : String\n",
    "        image url.\n",
    "    image2 : String\n",
    "        image url.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    htmlcode : HTML\n",
    "\n",
    "    \"\"\"\n",
    "    header = '''\n",
    "    <html>\n",
    "        <head>\n",
    "            <style>\n",
    "            *{box-sizing:border-box;}\n",
    "            .column{float:left;width:50%;padding:2px;}\n",
    "            /*Clearfix(clearfloats)*/\n",
    "            .row::after{content:\"\";clear:both;display:table;}\n",
    "            </style>\n",
    "        </head>\n",
    "    <body>\n",
    "    '''\n",
    "    footer = \"</body></html>\"\n",
    "    return HTML(\n",
    "        header +\n",
    "        '''\n",
    "        <div class=\"row\">\n",
    "            <div class=\"column\">\n",
    "            <img style=\"display: block;margin-left: auto;\n",
    "                        margin-right: auto;width: 60%;\"\n",
    "                 src=\"''' + IMAGE_PATH + image1 + '''\">\n",
    "            </div>\n",
    "            <div class=\"column\">\n",
    "            <img style=\"display: block;margin-left: auto;\n",
    "                        margin-right: auto;width: 60%;\"\n",
    "                 src=\"''' + IMAGE_PATH + image2 + '''\">\n",
    "            </div>\n",
    "        </div>\n",
    "        '''\n",
    "        + footer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_animation(filenames, duration=0.50, name='tmp'):\n",
    "    \"\"\"\n",
    "    Create gif animation method. \n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    filenames : array\n",
    "        Set of the images names to create the animation.\n",
    "    duration : float\n",
    "        Duration of one image in the animation.\n",
    "    name : String\n",
    "        Name used to save the animation to disk.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    gifname : String\n",
    "        Name of the animation file.\n",
    "\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for filename in filenames:\n",
    "        images.append(imageio.imread(IMAGE_PATH + filename))\n",
    "    gifname = name + \".gif\"\n",
    "    output_file = IMAGE_PATH + '%s' % gifname\n",
    "    imageio.mimsave(output_file, images, duration=duration)\n",
    "    return gifname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "Two problems from the Learning from data book were selected. To be precisly the problem **1.4** and **1.5**. The experiments will be executed using the functions implemented above. So if you have any doubt about the execution of the code, for example the meaning of any parameter, you can consult the comments in the code above. Each experiments consts of a set of sub experiments. Each sub experiments has at the beginning its description, followed by the code to execute it, then a grapth if is the case and finally an interpretation of the obtained results. Please note that sometimes you have to execute more than one cell to execute the whole sub experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLA, Problem 1.4\n",
    "\n",
    "In this Experiment, we use an artificial data set to study the perceptron learning algorithm . We are going to explore the algorithm further with data sets of different sizes and dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are first going to generate a linearly separable data set of size $n=20$ points. To generate the data set randomly we will use a radom seed `1995` so we can run it and get the same random result each time we execute it. It is important to notice that the data set generator is already included in the Percpetron class created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_b = Perceptron(20,datarseed=1995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are now going to run the perceptron learning algorithm on the data set above. Since the PLA is an iterative algorithm the main idea is to report the number of updates that the it takes before converging and observe how it performs with respect to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 11\n"
     ]
    }
   ],
   "source": [
    "iterations = p_b.pla(save=True, suffix='pb_')\n",
    "print(\"Number of iteratios: %d\" % (iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To cleary show what is going on with the algorithm execution we plot the examples $\\{ (X_n, Y_n) \\}$ , the target function $f$ (**black**), and the final hypothesis $g$ (**green**) in the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "            *{box-sizing:border-box;}\n",
       "            .column{float:left;width:50%;padding:2px;}\n",
       "            /*Clearfix(clearfloats)*/\n",
       "            .row::after{content:\"\";clear:both;display:table;}\n",
       "            </style>\n",
       "        </head>\n",
       "    <body>\n",
       "    \n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/animation_pb_1995.gif\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/pb_p_N20_it11.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = get_images('pb_')\n",
    "animation = create_animation(images,0.50,name='animation_pb_1995')\n",
    "show_animation(animation, images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first plot we can observe the behaviour of the PLA which passes through **11 iterations** before find an hypothesis which classifies the data correctly. In the second plot we may notice that although $g$ is very close to $f$ and it completely separtes the data set, they are not quite identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We repeat everything we've done above with another randomly, random seed `1996`, generated data set of size $n=20,$ to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_c = Perceptron(20,datarseed=1996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 22\n"
     ]
    }
   ],
   "source": [
    "iterations = p_c.pla(save=True, suffix='pc_')\n",
    "print(\"Number of iteratios: %d\" % (iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "            *{box-sizing:border-box;}\n",
       "            .column{float:left;width:50%;padding:2px;}\n",
       "            /*Clearfix(clearfloats)*/\n",
       "            .row::after{content:\"\";clear:both;display:table;}\n",
       "            </style>\n",
       "        </head>\n",
       "    <body>\n",
       "    \n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/animation_pc_1996.gif\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/pc_p_N20_it22.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = get_images('pc_')\n",
    "animation = create_animation(images,0.50,name='animation_pc_1996')\n",
    "show_animation(animation, images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first plot we can observe the behaviour of the PLA which passes through **22 iterations** before find an hypothesis which classifies the data correctly. In the second plot we may notice that although $g$ is very close to $f$ and it completely  separtes the data set , they are not quite identical. Compared with the previous results this time the PLA took more iterations to converge. We can observe that the PLA can take more or less iterations to converge depending on the data set and not in $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to analyse the behaviour of the PLA with respect to the data set size we repeat everything we've done in the first part of this experiment, with a nother randomly generated data set of size $n=100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_d = Perceptron(100,datarseed=7878)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 75\n"
     ]
    }
   ],
   "source": [
    "iterations = p_d.pla(save=True, suffix='pd_')\n",
    "print(\"Number of iteratios: %d\" % (iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "            *{box-sizing:border-box;}\n",
       "            .column{float:left;width:50%;padding:2px;}\n",
       "            /*Clearfix(clearfloats)*/\n",
       "            .row::after{content:\"\";clear:both;display:table;}\n",
       "            </style>\n",
       "        </head>\n",
       "    <body>\n",
       "    \n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/animation_pd_7878.gif\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/pd_p_N100_it75.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = get_images('pd_')\n",
    "animation = create_animation(images,0.50,name='animation_pd_7878')\n",
    "show_animation(animation, images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first plot we can observe the behaviour of the PLA which passes through **75 iterations** before converge. In the second plot we may notice that $g$ if pretty close to $f$ and it completely separtes the data set. Compared with the two previous results this time the PLA took more iterations than both. Also we can notice that this time $g$ is more similiar to $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to analyse the behaviour of the PLA with respect to the data set size we again repeat everything we've done in the first part of this experiment, with another randomly generated data set of size $n=1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_e = Perceptron(1000,datarseed=2000)\n",
    "p_e.plot(save=True, imgname='pe_sample_2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 432\n"
     ]
    }
   ],
   "source": [
    "iterations = p_e.pla()\n",
    "print(\"Number of iteratios: %d\" % (iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "            *{box-sizing:border-box;}\n",
       "            .column{float:left;width:50%;padding:2px;}\n",
       "            /*Clearfix(clearfloats)*/\n",
       "            .row::after{content:\"\";clear:both;display:table;}\n",
       "            </style>\n",
       "        </head>\n",
       "    <body>\n",
       "    \n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/pe_sample_2000.png\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/animation_pe_sample_2000.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_e.plot(vec=p_e.w.tolist(), save=True, imgname='animation_pe_sample_2000')\n",
    "show_animation('pe_sample_2000.png', 'animation_pe_sample_2000.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the animation of the PLA algorithm is not shown due to the number of iterations it takes, **432 iterations**. However the first plot shows $f$ and the second one shows $g$ and $f$ together which are nearly indistinguishable. Compared to the previos results we observe that the number of iterations grows when the data set is bigger, but also that $g$ is closer to $f$ since the possibilites to separate the data are fewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will now try to observe the performance of the PLA modifying the algorithm such that it takes $x_n  \\in  \\mathbb{R}^{10} $ instead of $\\mathbb{R}^2$ . A linearly separable data set of size $n=1000$ will be generated and we will let the PLA run until converge to count the number of iterations it takes to converge. It is important to mention that it is possible to specify the number of dimensions in which the algorithm works within the Perceptron class created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 4569\n"
     ]
    }
   ],
   "source": [
    "p_f = Perceptron(1000,datarseed=2003,dimension=10)\n",
    "iterations = p_f.pla()\n",
    "print(\"Number of iteratios: %d\" % (iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PLA took **4569 iterations** way higher than the previous results with $x \\in \\mathbb{R}^2$. So we can say that the increase on the dimensios has an significant impact in the PLA performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Repeat the algorithm on the same data set used above for 100 experiments. In the iterations of each experiment, pick $x(t)$ **randomly** instead of deterministically. Plot a histogram for the number of updates that the algorithm takes to converge.\n",
    "\n",
    "`Note:` Please consider that running this code below could take long time. If you want to run it please change the `run` boolean variable to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "aiter = []\n",
    "run = False # change to run 100 experiments\n",
    "g = random.Random()\n",
    "g.seed(8435)\n",
    "rs = g.sample(range(1000, 9999), 100)\n",
    "if run:\n",
    "    for i in range(100):\n",
    "        p_g = Perceptron(1000,datarseed=2003, rmispts=True, misptsrseed=rs[i], dimension=10)\n",
    "        iterations = p_g.pla()\n",
    "        aiter.append(iterations)\n",
    "        print(\"Number of iterations: %d, random seed %d\" % (iterations,rs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " |**Iterations**|**Random seed**| |**Iterations**|**Random seed**| |**Iterations**|**Random seed**| |**Iterations**|**Random seed**| |**Iterations**|**Random seed**\n",
    ":-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:\n",
    "**1.**|3207|5185|**21.**|2423|6375|**41.**|3391|6652|**61.**|3215|3084|**81.**|3407|9231\n",
    "**2.**|3711|1550|**22.**|3399|3469|**42.**|2673|8899|**62.**|2969|1391|**82.**|2701|9009\n",
    "**3.**|3717|7200|**23.**|2495|1138|**43.**|3511|1820|**63.**|3245|6176|**83.**|2895|3670\n",
    "**4.**|3623|8980|**24.**|3021|2746|**44.**|2279|7739|**64.**|2671|6455|**84.**|3329|2762\n",
    "**5.**|1971|6864|**25.**|3397|7554|**45.**|2435|2125|**65.**|2693|1755|**85.**|3409|3730\n",
    "**6.**|2960|1665|**26.**|3037|1620|**46.**|3503|7197|**66.**|2969|2935|**86.**|2535|4068\n",
    "**7.**|2437|4429|**27.**|2971|1574|**47.**|3127|1052|**67.**|3517|1260|**87.**|2311|5362\n",
    "**8.**|3907|6619|**28.**|3477|2468|**48.**|2363|1601|**68.**|2881|5693|**88.**|2251|5258\n",
    "**9.**|1959|9700|**29.**|2737|7188|**49.**|2681|5742|**69.**|2885|7089|**89.**|2827|1671\n",
    "**10.**|2487|4882|**30.**|2627|4789|**50.**|3205|4969|**70.**|3231|8424|**90.**|3417|4125\n",
    "**11.**|2367|4599|**31.**|3719|3910|**51.**|2843|1141|**71.**|3617|3684|**91.**|2399|6289\n",
    "**12.**|2501|9305|**32.**|3283|9501|**52.**|3107|8812|**72.**|2721|1355|**92.**|2669|5539\n",
    "**13.**|2913|9192|**33.**|2477|2956|**53.**|2851|8673|**73.**|2795|4975|**93.**|2399|9730\n",
    "**14.**|1999|9419|**34.**|2521|6595|**54.**|2669|3597|**74.**|2245|2371|**94.**|2283|1106\n",
    "**15.**|3651|9428|**35.**|1805|1196|**55.**|2859|2044|**75.**|2877|1182|**95.**|2857|7987\n",
    "**16.**|2653|8028|**36.**|1441|1007|**56.**|3645|7939|**76.**|3617|5816|**96.**|2177|1061\n",
    "**17.**|3263|5212|**37.**|3073|3354|**57.**|3011|8749|**77.**|3023|7602|**97.**|3005|9517\n",
    "**18.**|3313|9357|**38.**|2987|4439|**58.**|2647|9926|**78.**|3241|3753|**98.**|3329|7092\n",
    "**19.**|3137|1918|**39.**|2459|6847|**59.**|3245|9533|**79.**|2591|4191|**99.**|2993|1416\n",
    "**20.**|2885|3109|**40.**|3186|6934|**60.**|2783|1448|**80.**|2781|4462|**100.**|2523|8021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAFPCAYAAADqY80aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTr5mZhk4+ZqZZeDka2aWwf8CpcO4wmHf0R4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x233438e36a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array(aiter) if run else \\\n",
    "    np.array([3207,3711,3717,3623,1971,2960,2437,3907,1959,2487,\n",
    "              2367,2501,2913,1999,3651,2653,3263,3313,3137,2885,\n",
    "              2423,3399,2495,3021,3397,3037,2971,3477,2737,2627,\n",
    "              3719,3283,2477,2521,1805,1441,3073,2987,2459,3186,\n",
    "              3391,2673,3511,2279,2435,3503,3127,2363,2681,3205,\n",
    "              2843,3107,2851,2669,2859,3645,3011,2647,3245,2783,\n",
    "              3215,2969,3245,2671,2693,2969,3517,2881,2885,3231,\n",
    "              3617,2721,2795,2245,2877,3617,3023,3241,2591,2781,\n",
    "              3407,2701,2895,3329,3409,2535,2311,2251,2827,3417,\n",
    "              2399,2669,2399,2283,2857,2177,3005,3329,2993,2523])\n",
    "\n",
    "plt.gcf().clear()\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(x, 10, facecolor='blue', alpha=0.75)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Count')\n",
    "plt.title(r'$\\mathrm{Histogram\\ of\\ IQ:}\\ \\mu=' + str(x.mean()) + ',\\ \\sigma='+ str(x.std()) +'$')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary we have executed 100 time the a data set with $n=1000$ where $ x_n  \\in  \\mathbb{R}^{10} $ using different random seeds to select randomly the miscalssifed point. The results give us a mean of $\\mu = 2895.24$ and $\\sigma = 476.73$ which compared with previous experiment, in where we chose the misclassified point deterministically, needs in average, less iterations to converge. with a minimum of **1441** and maximum of **3907** iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expriment conclusions\n",
    "\n",
    "The main objectives of this experimet was to understand how the perceptron algorithm works and to observe how the amount of data and the data space affect its performance. So we can conclude that the more separable data we have the closer the hypothesis $g$ \n",
    "will be with respecto to $f$. However we could observe that if we choose deterministically the misclassified points  the number of iterations will increase. Another point to make is that, as we observed in the last two sub experiments, the dimension space of the data has a signficant impact on the number of iterations to classify the data, for instance with $x \\in \\mathbb{R}^2$ we needed 432 iterations while with $x \\in \\mathbb{R}^{10} $ the algorithm needed 4569 iterations, which is aproximetally 11 times more.\n",
    "\n",
    "Also we observed that instead of determinstically, we can choose the misclassified point randomly in the PLA, which performed better in an average of 100 trials with different random seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADALINE, Problem 1.5\n",
    "\n",
    "In this second implementation we are going to experiment with a varition of the PLA called ADALINE, which stands for ADaptative LInear NEuron. The main idea of this set of experiments is to train the algorithm varying the value of $\\eta$ which is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will first Generate a **training data set** of size 100 similar to that used in Exercise 1.4. Then we will generate a **test data set** of size 10,000 from the same process. To get $g$, we will run the ADALINE with $\\eta = 100$ on the training data set, until a maximum of 1,000 updates has been reached. We are going to plot the training data set, the target function $f$, and the final hypothesis $g$ and we will also report the error on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 1000.000000\n"
     ]
    }
   ],
   "source": [
    "pada_a = Perceptron(100, datarseed=1995, rmispts=True, misptsrseed=8955)\n",
    "\n",
    "iterations = pada_a.adaline(lrate=4, limit=1000)\n",
    "print(\"Number of iteratios: %f\" % (iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $\\eta = 100$ was not possible in this experiment since at certain point it started to generates too small values which produced Python to recognized it as a non-finite quantity (NaN). So we instead used the maximun value, $\\eta =4$ to avoid that scaneario. In part we had that issue because we are doing 1000 iterations so that means if we use a bigger learning rate we should nee less iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of misclassified points: 0.302700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "            *{box-sizing:border-box;}\n",
       "            .column{float:left;width:50%;padding:2px;}\n",
       "            /*Clearfix(clearfloats)*/\n",
       "            .row::after{content:\"\";clear:both;display:table;}\n",
       "            </style>\n",
       "        </head>\n",
       "    <body>\n",
       "    \n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/pada_4_10000.png\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/pada_4_miscpoints.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error, miscpoints, x = pada_a.check_error(10000,pada_a.w)\n",
    "print(\"fraction of misclassified points: %f\" % error)\n",
    "plt.gcf().clear()\n",
    "pada_a.plot(vec=pada_a.w.tolist(), save=True, imgname='pada_4_10000')\n",
    "plt.gcf().clear()\n",
    "pada_a.plot(mispts=miscpoints, save=True, imgname='pada_4_miscpoints')\n",
    "show_animation('pada_4_10000.png','pada_4_miscpoints.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we can see the two graphs above and observe that $g$ is quite far from $f$ which causes a **fraction of misclassified points = 0.302700** over the 10000 test points we used to test. The result is clearly what we expected since the $g$ is no quite near $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will Use the same data set used in the previous sub experiment and redo everything with $\\eta = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 48.000000\n"
     ]
    }
   ],
   "source": [
    "pada_b = Perceptron(100, datarseed=1995, rmispts=True, misptsrseed=8955)\n",
    "\n",
    "iterations = pada_b.adaline(lrate=1, limit=1000)\n",
    "print(\"Number of iteratios: %f\" % (iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of misclassified points: 0.007400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "            *{box-sizing:border-box;}\n",
       "            .column{float:left;width:50%;padding:2px;}\n",
       "            /*Clearfix(clearfloats)*/\n",
       "            .row::after{content:\"\";clear:both;display:table;}\n",
       "            </style>\n",
       "        </head>\n",
       "    <body>\n",
       "    \n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/pada_1_10000.png\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/pada_1_miscpoints.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error, miscpoints, x = pada_b.check_error(10000,pada_b.w)\n",
    "print(\"fraction of misclassified points: %f\" % error)\n",
    "plt.gcf().clear()\n",
    "pada_b.plot(vec=pada_b.w.tolist(), save=True, imgname='pada_1_10000')\n",
    "plt.gcf().clear()\n",
    "pada_b.plot(mispts=miscpoints, save=True, imgname='pada_1_miscpoints')\n",
    "show_animation('pada_1_10000.png','pada_1_miscpoints.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the algorithm with $\\eta = 1$ we just needed 48 iterations to separate the data. Testing the the 10000 points data set we got **fraction of misclassified points = 0.0074** which is way better than the previous test. As we can see in the graphs above this time $g$ is closer to $f$ and thus we have better results. In the graph to the right we can observe the missclassified points, 74 in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will now use the same data set used in the previous sub experiment and redo everything with $\\eta = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 603.000000\n"
     ]
    }
   ],
   "source": [
    "pada_c = Perceptron(100, datarseed=1995, rmispts=True, misptsrseed=8955)\n",
    "iterations = pada_c.adaline(lrate=0.01, limit=1000)\n",
    "\n",
    "print(\"Number of iteratios: %f\" % (iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of misclassified points: 0.003800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "            *{box-sizing:border-box;}\n",
       "            .column{float:left;width:50%;padding:2px;}\n",
       "            /*Clearfix(clearfloats)*/\n",
       "            .row::after{content:\"\";clear:both;display:table;}\n",
       "            </style>\n",
       "        </head>\n",
       "    <body>\n",
       "    \n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/pada_001_10000.png\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/pada_001_miscpoints.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error, miscpoints, x = pada_c.check_error(10000,pada_c.w)\n",
    "print(\"fraction of misclassified points: %f\" % error)\n",
    "plt.gcf().clear()\n",
    "pada_c.plot(vec=pada_c.w.tolist(), save=True, imgname='pada_001_10000')\n",
    "plt.gcf().clear()\n",
    "pada_c.plot(mispts=miscpoints, save=True, imgname='pada_001_miscpoints')\n",
    "show_animation('pada_001_10000.png','pada_001_miscpoints.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the algorithm with $\\eta = 0.01$ we needed 603 iterations to separate the data which is more than the previous test. However this time we have a better fit $g$ function, almost the same as $f$. Testing the the 10000 points data set we got **fraction of misclassified points = 0.0038** which is way better than the previous test. So at this point we observe that the lower the value of $\\eta$ the better results we got but also the more iterations it takes to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will now use the same data set used in the previous sub experiment and redo everything with $\\eta = 0.0001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 205.000000\n"
     ]
    }
   ],
   "source": [
    "pada_d = Perceptron(100, datarseed=1995, rmispts=True, misptsrseed=8955)\n",
    "iterations = pada_d.adaline(lrate=0.0001, limit=1000)\n",
    "\n",
    "print(\"Number of iteratios: %f\" % (iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of misclassified points: 0.007500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "            *{box-sizing:border-box;}\n",
       "            .column{float:left;width:50%;padding:2px;}\n",
       "            /*Clearfix(clearfloats)*/\n",
       "            .row::after{content:\"\";clear:both;display:table;}\n",
       "            </style>\n",
       "        </head>\n",
       "    <body>\n",
       "    \n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/pada_00001_10000.png\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;\n",
       "                        margin-right: auto;width: 60%;\"\n",
       "                 src=\"images/pada_00001_miscpoints.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error, miscpoints, x = pada_d.check_error(10000,pada_d.w)\n",
    "print(\"fraction of misclassified points: %f\" % error)\n",
    "plt.gcf().clear()\n",
    "pada_d.plot(vec=pada_d.w.tolist(), save=True, imgname='pada_00001_10000')\n",
    "plt.gcf().clear()\n",
    "pada_d.plot(mispts=miscpoints, save=True, imgname='pada_00001_miscpoints')\n",
    "show_animation('pada_00001_10000.png','pada_00001_miscpoints.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the algorithm with $\\eta = 0.0001$ we needed 205 iterations to separate the data which. We can observe that the higuer or lower $\\eta$ is the less iterations the algorithm will take to converge. We can see in the first graph that again $g$ is a little deviated from $f$ with **fraction of misclassified points = 0.0075**. We got similar results with $\\eta = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Expriment conclusions\n",
    "\n",
    "Analysing the experiments we just have done, we can observe that we obtained better results with a value of $\\eta$ that is not in the extremes but in the middle. So we can say that if we need to find a balance in the learning rate because it can push too much or too low to converge.\n",
    "\n",
    "We also can observe that the greater or lower $\\eta$ is the less iterations the algorithm will need to converge. And if $\\eta$ is moderate it will converge slower. We can observe that since the number of iterations for $\\eta=1$, $\\eta=0.01$ and $\\eta=0.0001$ converge **48**, **603** and **205** iterations respectivelly. So it is possible to adjust those parameters in order to try to get better results, but we noticed that for classification purposes more iterations is better than a lower or higher learning rate $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overal conclusion\n",
    "\n",
    "These algorithms implemented here are very fundamental ML algorithms knwon as linear classifiers. Through having executed the experiments we could oberve their performance in different scenarios: variations in dimensional space, amount of data and randomness at the moment of choose the missclasified point for the PLA. We also observed how the ADALINE algorithm performed with different learnig rates. What we can see is that in general the ADALINE algorithm performed better in terms of number of iterations to converge but we have to be careful about how to set the learning rate since selecting the wrong one can also give us very bad hypothesis $g$ meaning that it didn't aprroximate $f$ very well. However both algorithms are very similiar since given a set of lineraly separable data they asymptotically will find a hyptohesis that classifies the data.\n",
    "\n",
    "Another observations that apply to both algorithms, is the fact that the more data we have to find a hypothesis the closer the hypothesis will be to target function $f$. In fact the quiantity of data and its variety is a critical aspect of Machine learning in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Abu-Mostafa, Y. S., Magdon-Ismail, M., & Lin, H. T. (2012). Learning from data (Vol. 4). New York, NY, USA:: AMLBook.\n",
    "- [Adaline classifier](https://rasbt.github.io/mlxtend/user_guide/classifier/Adaline/).\n",
    "- [Adaline and perceptron](https://sebastianraschka.com/faq/docs/diff-perceptron-adaline-neuralnet.html).\n",
    "- [Linearly separable data](http://www.3dmatics.com/blog/2014/11/artificial-linearly-separable-test-data-in-python/)\n",
    "- [Notes on perceptron](http://www.3dmatics.com/blog/2014/11/notes-on-perceptron-part-1/)\n",
    "- [Notes on percetpron adaline](http://www.3dmatics.com/blog/2014/12/notes-on-perceptron-part-5-adaline/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
