{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we will study 2 different Machine Learning algorithms **Perceptron Learning Algorithm (PLA)** and its variant **ADaptative LInear NEuron (ADALINE)**. We will do that through doing a set of experiments for each algorithm and analyzing its results to finally discuss the main features of each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning from data\n",
    "<img src=\"images/lfd.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write and Introduction of ML according to the Book\n",
    "\n",
    "\n",
    "- Write a paragraph talking about the different ML algorithms\n",
    "\n",
    "\n",
    "- Describe the PLA algorithm\n",
    "\n",
    "\n",
    "- Describe the Adaline algorithm\n",
    "\n",
    "\n",
    "- Compare both algorithms\n",
    "\n",
    "\n",
    "- Explain the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Libraries needed to implement and visualize the PLA algorithm and its variant \n",
    "Adaline.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import rcParams\n",
    "import imageio\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Images path\n",
    "IMAGE_PATH = 'images/'\n",
    "\n",
    "# setting up the figure size\n",
    "rcParams[\"figure.figsize\"] = 5, 5\n",
    "rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    PLA, Perceptron Learning Algorithm and Adaline, Adaptive Linear Neuron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        N : Integer\n",
    "            Number of linearly separable points to generate.\n",
    "        datarseed : Integer\n",
    "            Seed used to initialize the datas random generator.\n",
    "            Default: 1324\n",
    "        rmispts : Boolean\n",
    "            If True the misclassified points are selected randomly.\n",
    "            Default: False\n",
    "        misptsrseed : Integer\n",
    "            Seed used to initialize the misclasifications random generator.\n",
    "            Default: 1324\n",
    "        dimension : Integer\n",
    "            Dimension in which the points are generated.\n",
    "            Default: 2\n",
    "    Attributes\n",
    "    ----------\n",
    "        w : array\n",
    "            Weights vector.\n",
    "        X : array, shape = [nsamples, nfeatures]\n",
    "            Training vectors, where 'nasamples' is the number of\n",
    "            samples and 'nfeatures' is the number of features.\n",
    "        V : array\n",
    "            Line in 2d or n-plane in n-dimensions that separates the points.\n",
    "        dim : array\n",
    "            Dimension in which the algorithm works.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, N, datarseed=1324, rmispts=False, misptsrseed=1234, dimension=2\n",
    "    ):\n",
    "\n",
    "        self.dim = dimension\n",
    "        self.rgendata = np.random.RandomState(datarseed)\n",
    "        self.rgenmispts = np.random.RandomState(misptsrseed)\n",
    "        self.rmispts = rmispts\n",
    "\n",
    "        # Random linearly separated data,\n",
    "        var = [self.rgendata.uniform(-1, 1) for i in range(self.dim * 2)]\n",
    "        for i in range(self.dim + 1):\n",
    "            if i == 0:\n",
    "                self.V = np.array(\n",
    "                    var[i + 2] * var[i + 1] - var[i] * var[i + 3])\n",
    "            elif i % 2 == 0:\n",
    "                self.V = np.append(self.V, var[i] - var[i - 1])\n",
    "            else:\n",
    "                self.V = np.append(self.V, var[i])\n",
    "\n",
    "        self.X = self.generate_points(N)\n",
    "\n",
    "    def generate_points(self, N):\n",
    "        \"\"\"\n",
    "        Generate linearly separable data points method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        N : Integer\n",
    "            Number of data points to generate.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        X : array, shape = [nsamples, nfeatures]\n",
    "            Training vectors, where 'nasamples' is the number of\n",
    "            samples and 'nfeatures' is the number of features.\n",
    "\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        for i in range(N):\n",
    "            xn = [self.rgendata.uniform(-1, 1) for i in range(self.dim)]\n",
    "            x = np.append([1], xn)\n",
    "            s = int(np.sign(self.V.T.dot(x)))\n",
    "            X.append((x, s))\n",
    "        return X\n",
    "\n",
    "    def plot(self, mispts=None, vec=None, save=False, imgname='tmp'):\n",
    "        \"\"\"\n",
    "        2D data plot method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        mispts : array\n",
    "            Set of misclassified data points.\n",
    "            Default: None\n",
    "        vec : array\n",
    "            Weights vector that describes the hypotesis.\n",
    "            Default: None\n",
    "        save : Boolean\n",
    "            If the value is True the plots are saved to disk.\n",
    "            Default: False\n",
    "        imgname : String\n",
    "            Name used to save the plot to disk.\n",
    "            Default: 'tmp'\n",
    "\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        plt.xlim(-1, 1)\n",
    "        plt.ylim(-1, 1)\n",
    "        V = self.V\n",
    "\n",
    "        a, b = -V[1] / V[2], -V[0] / V[2]\n",
    "        l = np.linspace(-1, 1)\n",
    "\n",
    "        plt.plot(l, a * l + b, 'k-')\n",
    "        cols = {1: 'r', -1: 'b'}\n",
    "\n",
    "        if mispts == None:\n",
    "            for x, s in self.X:\n",
    "                plt.plot(x[1], x[2], cols[s] + 'o')\n",
    "\n",
    "        if mispts:\n",
    "            for x, s in mispts:\n",
    "                plt.plot(x[1], x[2], cols[s] + '.')\n",
    "\n",
    "        if vec != None:\n",
    "            aa, bb = -vec[1] / vec[2], -vec[0] / vec[2]\n",
    "            plt.plot(l, aa * l + bb, 'g-', lw=2)\n",
    "\n",
    "        if save:\n",
    "            if not mispts:\n",
    "                plt.title('N = %s' % (str(len(self.X))))\n",
    "                plt.xlabel('x1')\n",
    "                plt.ylabel('x2')\n",
    "            else:\n",
    "                plt.title(\n",
    "                    'N = %s with %s test points' %\n",
    "                    (str(len(self.X)), str(len(mispts)))\n",
    "                )\n",
    "                plt.xlabel('x1')\n",
    "                plt.ylabel('x2')\n",
    "            plt.savefig(\n",
    "                IMAGE_PATH + imgname, dpi=70, bbox_inches='tight'\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "    def classification_error(self, vec, pts=None):\n",
    "        \"\"\"\n",
    "        Error classification method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        vec : array\n",
    "            Weights vector used to classify a point.\n",
    "        pts : array\n",
    "            Data points to evaluate:\n",
    "            Default: self.X\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        error : float\n",
    "            Fraction of misclassified points.\n",
    "        mispts: array\n",
    "            Set of misclassified points.\n",
    "\n",
    "        \"\"\"\n",
    "        # Error defined as fraction of misclassified points\n",
    "        if not pts:\n",
    "            pts = self.X\n",
    "        M = len(pts)\n",
    "        n_mispts = 0\n",
    "        mispts = []\n",
    "        for x, s in pts:\n",
    "            if int(np.sign(vec.T.dot(x))) != s:\n",
    "                n_mispts += 1\n",
    "                mispts.append((x, s))\n",
    "        error = n_mispts / float(M)\n",
    "        return error, mispts\n",
    "\n",
    "    def choose_miscl_point(self, vec):\n",
    "        \"\"\"\n",
    "        Choose a misclassified point method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        vec : array\n",
    "            Weights vector used to classify a point.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        mispts[i] : array\n",
    "            Selected misclassified data point.\n",
    "\n",
    "        \"\"\"\n",
    "        # Choose a random point among the misclassified\n",
    "        pts = self.X\n",
    "        mispts = []\n",
    "        for x, s in pts:\n",
    "            if int(np.sign(vec.T.dot(x))) != s:\n",
    "                if not self.rmispts:\n",
    "                    return (x, s)\n",
    "                mispts.append((x, s))\n",
    "        return mispts[self.rgenmispts.randint(0, len(mispts))]\n",
    "\n",
    "    def pla(self, save=False, suffix=''):\n",
    "        \"\"\"\n",
    "        Perceptron learning algorithm method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        save : Boolean\n",
    "            If value is True Save the sequence of plots in which the algorithm \n",
    "            go through.\n",
    "        suffix : String\n",
    "            suffix to add to the name used to save the sequence of plots to disk.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        it : Integer\n",
    "            Number of iterations the algorithm took to converge.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the weights to zeros\n",
    "        w = np.zeros(self.dim + 1)\n",
    "        X, N = self.X, len(self.X)\n",
    "\n",
    "        it = 0\n",
    "        # Iterate until all points are correctly classified\n",
    "        while self.classification_error(w)[0] != 0:\n",
    "            it += 1\n",
    "            # Pick random misclassified point\n",
    "            x, s = self.choose_miscl_point(w)\n",
    "            # Update weights\n",
    "            w += s * x\n",
    "            if save:\n",
    "                self.plot(vec=w.tolist())\n",
    "                plt.title('N = %s, Iteration %s\\n' % (str(N), str(it)))\n",
    "                plt.xlabel('x1')\n",
    "                plt.ylabel('x2')\n",
    "                plt.savefig(\n",
    "                    IMAGE_PATH + suffix + 'p_N%s_it%s' % (str(N), str(it)),\n",
    "                    dpi=70, bbox_inches='tight'\n",
    "                )\n",
    "                plt.close()\n",
    "        self.w = w\n",
    "        return it\n",
    "\n",
    "    def adaline(self, save=False, suffix='', lrate=1, limit=10):\n",
    "        \"\"\"\n",
    "        Adaptive linear neuron algorithm method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        save : Boolean\n",
    "            If value is True Save the sequence of plots in which the algorithm \n",
    "            go through.\n",
    "        suffix : String\n",
    "            suffix to add to the name used to save the sequence of plots to disk.\n",
    "        lrate : float\n",
    "            Learning rate factor.\n",
    "        limit : Integer\n",
    "            Maximum number of updates the algorithm does.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize the weigths to zeros\n",
    "        w = np.zeros(self.dim + 1)\n",
    "        X, N = self.X, len(self.X)\n",
    "\n",
    "        it = 0\n",
    "        # Iterate until all points are correctly classified\n",
    "        while it < limit and self.classification_error(w)[0] != 0:\n",
    "            it += 1\n",
    "            # Pick random misclassified point\n",
    "            x, s = self.choose_miscl_point(w)\n",
    "            y = w.T.dot(x)\n",
    "            # Update weights\n",
    "            w += lrate * (s - y) * x\n",
    "            if save:\n",
    "                self.plot(vec=w.tolist())\n",
    "                plt.title('N = %s, Iteration %s\\n' % (str(N), str(it)))\n",
    "                plt.xlabel('x1')\n",
    "                plt.ylabel('x2')\n",
    "                plt.savefig(\n",
    "                    IMAGE_PATH + suffix + 'p_N%s_it%s' % (str(N), str(it)),\n",
    "                    dpi=70, bbox_inches='tight'\n",
    "                )\n",
    "                plt.close()\n",
    "        self.w = w\n",
    "        return it\n",
    "\n",
    "    def check_error(self, M, vec):\n",
    "        \"\"\"\n",
    "        Test data error check method.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        N : Integer\n",
    "            Number of data points to generate.\n",
    "        vec : array\n",
    "            Weights vector, generated by running the algorithm with a training\n",
    "            set, used to classify the data points.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        error : float\n",
    "            Fraction of misclassified points.\n",
    "        miscpoints: array\n",
    "            Set of misclassified points.\n",
    "        check_pts: array\n",
    "            The generated data points.\n",
    "\n",
    "        \"\"\"\n",
    "        check_pts = self.generate_points(M)\n",
    "        error, miscpoints = self.classification_error(vec, pts=check_pts)\n",
    "        return error, miscpoints, check_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_images(suffix):\n",
    "    \"\"\"\n",
    "    Get the list of images associated to a suffix.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    suffix : String\n",
    "        Suffix of set of requested images.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    sortpngs : array\n",
    "        Set of images (plots) names.\n",
    "\n",
    "    \"\"\"\n",
    "    basedir = os.getcwd()\n",
    "    os.chdir(basedir)\n",
    "    pngs = [pl for pl in os.listdir(IMAGE_PATH) if pl.endswith(\n",
    "        'png') and pl.startswith(suffix)]\n",
    "    sortpngs = sorted(pngs, key=lambda a: int(a.split('it')[1][:-4]))\n",
    "    return sortpngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_animation(image1, image2):\n",
    "    \"\"\"\n",
    "    Show images or animations method.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    image1 : String\n",
    "        image url.\n",
    "    image2 : String\n",
    "        image url.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    htmlcode : HTML\n",
    "\n",
    "    \"\"\"\n",
    "    header = \"<html><head><style>*{box-sizing:border-box;}.column{float:left;width:50%;padding:2px;}/*Clearfix(clearfloats)*/.row::after{content:\"\";clear:both;display:table;}</style></head><body>\"\n",
    "    footer = \"</body></html>\"\n",
    "    return HTML(\n",
    "        header +\n",
    "        '''\n",
    "        <div class=\"row\">\n",
    "            <div class=\"column\">\n",
    "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"''' + IMAGE_PATH + image1 + '''\">\n",
    "            </div>\n",
    "            <div class=\"column\">\n",
    "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"''' + IMAGE_PATH + image2 + '''\">\n",
    "            </div>\n",
    "        </div>\n",
    "        '''\n",
    "        + footer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_animation(filenames, duration=0.50, name='tmp'):\n",
    "    \"\"\"\n",
    "    Create gif animation method. \n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    filenames : array\n",
    "        Set of the images names to create the animation.\n",
    "    duration : float\n",
    "        Duration of one image in the animation.\n",
    "    name : String\n",
    "        Name used to save the animation to disk.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    gifname : String\n",
    "        Name of the animation file.\n",
    "\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for filename in filenames:\n",
    "        images.append(imageio.imread(IMAGE_PATH + filename))\n",
    "    gifname = name + \".gif\"\n",
    "    output_file = IMAGE_PATH + '%s' % gifname\n",
    "    imageio.mimsave(output_file, images, duration=duration)\n",
    "    return gifname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLA, (Problem 1.4  Learning from data book)\n",
    "\n",
    "In this Experiment, we use an artificial data set to study the perceptron learning algorithm . We are going to explore the algorithm further with data sets of different sizes and dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are first going to generate a linearly separable data set of size $n=20$ points. To generate the data set randomly we will use a radom seed `1995` so we can run it and get the same random result each time we execute it. It is important to notice that the data set generator is already included in the Percpetron class created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_b = Perceptron(20,datarseed=1995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are now going to run the perceptron learning algorithm on the data set above. Since the PLA is an iterative algorithm the main idea is to report the number of updates that the it takes before converging and observe how it performs with respect to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 11\n"
     ]
    }
   ],
   "source": [
    "iterations = p_b.pla(save=True, suffix='pb_')\n",
    "print(\"Number of iteratios: %d\" % (iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To cleary show what is going on with the algorithm execution we plot the examples $\\{ (X_n, Y_n) \\}$ , the target function $f$ (**black**), and the final hypothesis $g$ (**green**) in the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><head><style>*{box-sizing:border-box;}.column{float:left;width:50%;padding:2px;}/*Clearfix(clearfloats)*/.row::after{content:;clear:both;display:table;}</style></head><body>\n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/animation_pb_1995.gif\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/pb_p_N20_it11.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = get_images('pb_')\n",
    "animation = create_animation(images,0.50,name='animation_pb_1995')\n",
    "show_animation(animation, images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first plot we can observe the behaviour of the PLA which passes through **11 iterations** before find an hypothesis which classifies the data correctly. In the second plot we may notice that although $g$ is very close to $f$ and it completely separtes the data set, they are not quite identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We repeat everything we've done above with another randomly, random seed `1996`, generated data set of size $n=20,$ to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_c = Perceptron(20,datarseed=1996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 22\n"
     ]
    }
   ],
   "source": [
    "iterations = p_c.pla(save=True, suffix='pc_')\n",
    "print(\"Number of iteratios: %d\" % (iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><head><style>*{box-sizing:border-box;}.column{float:left;width:50%;padding:2px;}/*Clearfix(clearfloats)*/.row::after{content:;clear:both;display:table;}</style></head><body>\n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/animation_pc_1996.gif\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/pc_p_N20_it22.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = get_images('pc_')\n",
    "animation = create_animation(images,0.50,name='animation_pc_1996')\n",
    "show_animation(animation, images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first plot we can observe the behaviour of the PLA which passes through **22 iterations** before find an hypothesis which classifies the data correctly. In the second plot we may notice that although $g$ is very close to $f$ and it completely  separtes the data set , they are not quite identical. Compared with the previous results this time the PLA took more iterations to converge. We can observe that the PLA can take more or less iterations to converge depending on the data set and not in $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to analyse the behaviour of the PLA with respect to the data set size we repeat everything we've done in the first part of this experiment, with a nother randomly generated data set of size $n=100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_d = Perceptron(100,datarseed=7878)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 75\n"
     ]
    }
   ],
   "source": [
    "iterations = p_d.pla(save=True, suffix='pd_')\n",
    "print(\"Number of iteratios: %d\" % (iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><head><style>*{box-sizing:border-box;}.column{float:left;width:50%;padding:2px;}/*Clearfix(clearfloats)*/.row::after{content:;clear:both;display:table;}</style></head><body>\n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/animation_pd_7878.gif\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/pd_p_N100_it75.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = get_images('pd_')\n",
    "animation = create_animation(images,0.50,name='animation_pd_7878')\n",
    "show_animation(animation, images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first plot we can observe the behaviour of the PLA which passes through **75 iterations** before converge. In the second plot we may notice that $g$ if pretty close to $f$ and it completely separtes the data set. Compared with the two previous results this time the PLA took more iterations than both. Also we can notice that this time $g$ is more similiar to $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to analyse the behaviour of the PLA with respect to the data set size we again repeat everything we've done in the first part of this experiment, with another randomly generated data set of size $n=1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_e = Perceptron(1000,datarseed=2000)\n",
    "p_e.plot(save=True, imgname='pe_sample_2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 432\n"
     ]
    }
   ],
   "source": [
    "iterations = p_e.pla()\n",
    "print(\"Number of iteratios: %d\" % (iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><head><style>*{box-sizing:border-box;}.column{float:left;width:50%;padding:2px;}/*Clearfix(clearfloats)*/.row::after{content:;clear:both;display:table;}</style></head><body>\n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/pe_sample_2000.png\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/animation_pe_sample_2000.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_e.plot(vec=p_e.w.tolist(), save=True, imgname='animation_pe_sample_2000')\n",
    "show_animation('pe_sample_2000.png', 'animation_pe_sample_2000.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the animation of the PLA algorithm is not shown due to the number of iterations it takes, **432 iterations**. However the first plot shows $f$ and the second one shows $g$ and $f$ together which are nearly indistinguishable. Compared to the previos results we observe that the number of iterations grows when the data set is bigger, but also that $g$ is closer to $f$ since the possibilites to separate the data are fewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will now try to observe the performance of the PLA modifying the algorithm such that it takes $x_n  \\in  \\mathbb{R}^{10} $ instead of $\\mathbb{R}^2$ . A linearly separable data set of size $n=1000$ will be generated and we will let the PLA run until converge to count the number of iterations it takes to converge. It is important to mention that it is possible to specify the number of dimensions in which the algorithm works within the Perceptron class created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 4569\n"
     ]
    }
   ],
   "source": [
    "p_f = Perceptron(1000,datarseed=2003,dimension=10)\n",
    "iterations = p_f.pla()\n",
    "print(\"Number of iteratios: %d\" % (iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PLA took **4569 iterations** way higher than the previous results with $x \\in \\mathbb{R}^2$. So we can say that the increase on the dimensios has an significant impact in the PLA performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Repeat the algorithm on the same data set used above for 100 experiments. In the iterations of each experiment, pick $x(t)$ **randomly** instead of deterministically. Plot a histogram for the number of updates that the algorithm takes to converge.\n",
    "\n",
    "`Note:` Please consider that running this code below could take long time. If you want to run it please change the `run` boolean variable to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "aiter = []\n",
    "run = False # change to run 100 experiments\n",
    "g = random.Random()\n",
    "g.seed(8435)\n",
    "rs = g.sample(range(1000, 9999), 100)\n",
    "if run:\n",
    "    for i in range(100):\n",
    "        p_g = Perceptron(1000,datarseed=2003, rmispts=True, misptsrseed=rs[i], dimension=10)\n",
    "        iterations = p_g.pla()\n",
    "        aiter.append(iterations)\n",
    "        print(\"Number of iterations: %d, random seed %d\" % (iterations,rs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " |**Iterations**|**Random seed**| |**Iterations**|**Random seed**| |**Iterations**|**Random seed**| |**Iterations**|**Random seed**| |**Iterations**|**Random seed**\n",
    ":-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:\n",
    "**1.**|3207|5185|**21.**|2423|6375|**41.**|3391|6652|**61.**|3215|3084|**81.**|3407|9231\n",
    "**2.**|3711|1550|**22.**|3399|3469|**42.**|2673|8899|**62.**|2969|1391|**82.**|2701|9009\n",
    "**3.**|3717|7200|**23.**|2495|1138|**43.**|3511|1820|**63.**|3245|6176|**83.**|2895|3670\n",
    "**4.**|3623|8980|**24.**|3021|2746|**44.**|2279|7739|**64.**|2671|6455|**84.**|3329|2762\n",
    "**5.**|1971|6864|**25.**|3397|7554|**45.**|2435|2125|**65.**|2693|1755|**85.**|3409|3730\n",
    "**6.**|2960|1665|**26.**|3037|1620|**46.**|3503|7197|**66.**|2969|2935|**86.**|2535|4068\n",
    "**7.**|2437|4429|**27.**|2971|1574|**47.**|3127|1052|**67.**|3517|1260|**87.**|2311|5362\n",
    "**8.**|3907|6619|**28.**|3477|2468|**48.**|2363|1601|**68.**|2881|5693|**88.**|2251|5258\n",
    "**9.**|1959|9700|**29.**|2737|7188|**49.**|2681|5742|**69.**|2885|7089|**89.**|2827|1671\n",
    "**10.**|2487|4882|**30.**|2627|4789|**50.**|3205|4969|**70.**|3231|8424|**90.**|3417|4125\n",
    "**11.**|2367|4599|**31.**|3719|3910|**51.**|2843|1141|**71.**|3617|3684|**91.**|2399|6289\n",
    "**12.**|2501|9305|**32.**|3283|9501|**52.**|3107|8812|**72.**|2721|1355|**92.**|2669|5539\n",
    "**13.**|2913|9192|**33.**|2477|2956|**53.**|2851|8673|**73.**|2795|4975|**93.**|2399|9730\n",
    "**14.**|1999|9419|**34.**|2521|6595|**54.**|2669|3597|**74.**|2245|2371|**94.**|2283|1106\n",
    "**15.**|3651|9428|**35.**|1805|1196|**55.**|2859|2044|**75.**|2877|1182|**95.**|2857|7987\n",
    "**16.**|2653|8028|**36.**|1441|1007|**56.**|3645|7939|**76.**|3617|5816|**96.**|2177|1061\n",
    "**17.**|3263|5212|**37.**|3073|3354|**57.**|3011|8749|**77.**|3023|7602|**97.**|3005|9517\n",
    "**18.**|3313|9357|**38.**|2987|4439|**58.**|2647|9926|**78.**|3241|3753|**98.**|3329|7092\n",
    "**19.**|3137|1918|**39.**|2459|6847|**59.**|3245|9533|**79.**|2591|4191|**99.**|2993|1416\n",
    "**20.**|2885|3109|**40.**|3186|6934|**60.**|2783|1448|**80.**|2781|4462|**100.**|2523|8021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAFPCAYAAADqY80aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTr5mZhk4+ZqZZeDka2aWwf8CpcO4wmHf0R4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x233e352d240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array(aiter) if run else \\\n",
    "    np.array([3207,3711,3717,3623,1971,2960,2437,3907,1959,2487,\n",
    "              2367,2501,2913,1999,3651,2653,3263,3313,3137,2885,\n",
    "              2423,3399,2495,3021,3397,3037,2971,3477,2737,2627,\n",
    "              3719,3283,2477,2521,1805,1441,3073,2987,2459,3186,\n",
    "              3391,2673,3511,2279,2435,3503,3127,2363,2681,3205,\n",
    "              2843,3107,2851,2669,2859,3645,3011,2647,3245,2783,\n",
    "              3215,2969,3245,2671,2693,2969,3517,2881,2885,3231,\n",
    "              3617,2721,2795,2245,2877,3617,3023,3241,2591,2781,\n",
    "              3407,2701,2895,3329,3409,2535,2311,2251,2827,3417,\n",
    "              2399,2669,2399,2283,2857,2177,3005,3329,2993,2523])\n",
    "\n",
    "plt.gcf().clear()\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(x, 10, facecolor='blue', alpha=0.75)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Count')\n",
    "plt.title(r'$\\mathrm{Histogram\\ of\\ IQ:}\\ \\mu=' + str(x.mean()) + ',\\ \\sigma='+ str(x.std()) +'$')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary we have executed 100 time the a data set with $n=1000$ where $ x_n  \\in  \\mathbb{R}^{10} $ using different random seeds to select randomly the miscalssifed point. The results give us a mean of $\\mu = 2895.24$ and $\\sigma = 476.73$ which compared with previous experiment, in where we chose the misclassified point deterministically, needs in average, less iterations to converge. with a minimum of **1441** and maximum of **3907** iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expriment conclusions\n",
    "\n",
    "The main objectives of this experimet was to understand how the perceptron algorithm works and to observe how the amount of data and the data space affect its performance. So we can conclude that the more separable data we have the closer the hypothesis $g$ \n",
    "will be with respecto to $f$. However we could observe that if we choose deterministically the misclassified points  the number of iterations will increase. Another point to make is that, as we observed in the last two sub experiments, the dimension space of the data has a signficant impact on the number of iterations to classify the data, for instance with $x \\in \\mathbb{R}^2$ we needed 432 iterations while with $x \\in \\mathbb{R}^{10} $ the algorithm needed 4569 iterations, which is aproximetally 11 times more.\n",
    "\n",
    "Also we observed that instead of determinstically, we can choose the misclassified point randomly in the PLA, which performed better in an average of 100 trials with different random seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADALINE, Problem 1.5\n",
    "\n",
    "In this second implementation we are going to experiment with a varition of the PLA called ADALINE, which stands for ADaptative LInear NEuron. The main idea of this set of experiments is to train the algorithm varying the value of $\\eta$ which is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will first Generate a **training data set** of size 100 similar to that used in Exercise 1.4. Then we will generate a **test data set** of size 10,000 from the same process. To get $g$, we will run the ADALINE with $\\eta = 100$ on the training data set, until a maximum of 1,000 updates has been reached. We are going to plot the training data set, the target function $f$, and the final hypothesis $g$ and we will also report the error on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 1000.000000\n"
     ]
    }
   ],
   "source": [
    "pada_a = Perceptron(100, datarseed=1995, rmispts=True, misptsrseed=8955)\n",
    "\n",
    "iterations = pada_a.adaline(lrate=4, limit=1000)\n",
    "print(\"Number of iteratios: %f\" % (iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $\\eta = 100$ was not possible in this experiment since at certain point it started to generates too small values which produced Python to recognized it as a non-finite quantity (NaN). So we instead used the maximun value, $\\eta =4$ to avoid that scaneario. In part we had that issue because we are doing 1000 iterations so that means if we use a bigger learning rate we should nee less iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of misclassified points: 0.302700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html><head><style>*{box-sizing:border-box;}.column{float:left;width:50%;padding:2px;}/*Clearfix(clearfloats)*/.row::after{content:;clear:both;display:table;}</style></head><body>\n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/pada_4_10000.png\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/pada_4_miscpoints.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error, miscpoints, x = pada_a.check_error(10000,pada_a.w)\n",
    "print(\"fraction of misclassified points: %f\" % error)\n",
    "plt.gcf().clear()\n",
    "pada_a.plot(vec=pada_a.w.tolist(), save=True, imgname='pada_4_10000')\n",
    "plt.gcf().clear()\n",
    "pada_a.plot(mispts=miscpoints, save=True, imgname='pada_4_miscpoints')\n",
    "show_animation('pada_4_10000.png','pada_4_miscpoints.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we can see the two graphs above and observe that $g$ is quite far from $f$ which causes a **fraction of misclassified points = 0.302700** over the 10000 test points we used to test. The result is clearly what we expected since the $g$ is no quite near $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will Use the same data set used in the previous sub experiment and redo everything with $\\eta = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 48.000000\n"
     ]
    }
   ],
   "source": [
    "pada_b = Perceptron(100, datarseed=1995, rmispts=True, misptsrseed=8955)\n",
    "\n",
    "iterations = pada_b.adaline(lrate=1, limit=1000)\n",
    "print(\"Number of iteratios: %f\" % (iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of misclassified points: 0.007400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html><head><style>*{box-sizing:border-box;}.column{float:left;width:50%;padding:2px;}/*Clearfix(clearfloats)*/.row::after{content:;clear:both;display:table;}</style></head><body>\n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/pada_1_10000.png\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/pada_1_miscpoints.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error, miscpoints, x = pada_b.check_error(10000,pada_b.w)\n",
    "print(\"fraction of misclassified points: %f\" % error)\n",
    "plt.gcf().clear()\n",
    "pada_b.plot(vec=pada_b.w.tolist(), save=True, imgname='pada_1_10000')\n",
    "plt.gcf().clear()\n",
    "pada_b.plot(mispts=miscpoints, save=True, imgname='pada_1_miscpoints')\n",
    "show_animation('pada_1_10000.png','pada_1_miscpoints.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the algorithm with $\\eta = 1$ we just needed 48 iterations to separate the data. Testing the the 10000 points data set we got **fraction of misclassified points = 0.0074** which is way better than the previous test. As we can see in the graphs above this time $g$ is closer to $f$ and thus we have better results. In the graph to the right we can observe the missclassified points, 74 in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will now use the same data set used in the previous sub experiment and redo everything with $\\eta = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 603.000000\n"
     ]
    }
   ],
   "source": [
    "pada_c = Perceptron(100, datarseed=1995, rmispts=True, misptsrseed=8955)\n",
    "iterations = pada_c.adaline(lrate=0.01, limit=1000)\n",
    "\n",
    "print(\"Number of iteratios: %f\" % (iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of misclassified points: 0.003800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html><head><style>*{box-sizing:border-box;}.column{float:left;width:50%;padding:2px;}/*Clearfix(clearfloats)*/.row::after{content:;clear:both;display:table;}</style></head><body>\n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/pada_001_10000.png\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/pada_001_miscpoints.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error, miscpoints, x = pada_c.check_error(10000,pada_c.w)\n",
    "print(\"fraction of misclassified points: %f\" % error)\n",
    "plt.gcf().clear()\n",
    "pada_c.plot(vec=pada_c.w.tolist(), save=True, imgname='pada_001_10000')\n",
    "plt.gcf().clear()\n",
    "pada_c.plot(mispts=miscpoints, save=True, imgname='pada_001_miscpoints')\n",
    "show_animation('pada_001_10000.png','pada_001_miscpoints.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the algorithm with $\\eta = 0.01$ we needed 603 iterations to separate the data which is more than the previous test. However this time we have a better fit $g$ function, almost the same as $f$. Testing the the 10000 points data set we got **fraction of misclassified points = 0.0038** which is way better than the previous test. So at this point we observe that the lower the value of $\\eta$ the better results we got but also the more iterations it takes to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will now use the same data set used in the previous sub experiment and redo everything with $\\eta = 0.0001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteratios: 205.000000\n"
     ]
    }
   ],
   "source": [
    "pada_d = Perceptron(100, datarseed=1995, rmispts=True, misptsrseed=8955)\n",
    "iterations = pada_d.adaline(lrate=0.0001, limit=1000)\n",
    "\n",
    "print(\"Number of iteratios: %f\" % (iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of misclassified points: 0.007500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html><head><style>*{box-sizing:border-box;}.column{float:left;width:50%;padding:2px;}/*Clearfix(clearfloats)*/.row::after{content:;clear:both;display:table;}</style></head><body>\n",
       "        <div class=\"row\">\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/pada_00001_10000.png\">\n",
       "            </div>\n",
       "            <div class=\"column\">\n",
       "            <img style=\"display: block;margin-left: auto;margin-right: auto;width: 65%;\" src=\"images/pada_00001_miscpoints.png\">\n",
       "            </div>\n",
       "        </div>\n",
       "        </body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error, miscpoints, x = pada_d.check_error(10000,pada_d.w)\n",
    "print(\"fraction of misclassified points: %f\" % error)\n",
    "plt.gcf().clear()\n",
    "pada_d.plot(vec=pada_d.w.tolist(), save=True, imgname='pada_00001_10000')\n",
    "plt.gcf().clear()\n",
    "pada_d.plot(mispts=miscpoints, save=True, imgname='pada_00001_miscpoints')\n",
    "show_animation('pada_00001_10000.png','pada_00001_miscpoints.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the algorithm with $\\eta = 0.0001$ we needed 205 iterations to separate the data which. We can observe that the higuer or lower $\\eta$ is the less iterations the algorithm will take to converge. We can see in the first graph that again $g$ is a little deviated from $f$ with **fraction of misclassified points = 0.0075**. We got similar results with $\\eta = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Expriment conclusions\n",
    "\n",
    "Analysing the experiments we just have done, we can observe that we obtained better results with a value of $\\eta$ that is not in the extremes but in the middle. So we can say that if we need to find a balance in the learning rate because it can push too much or too low to converge.\n",
    "\n",
    "We also can observe that the greater or lower $\\eta$ is the less iterations the algorithm will need to converge. And if $\\eta$ is moderate it will converge slower. We can observe that since the number of iterations for $\\eta=1$, $\\eta=0.01$ and $\\eta=0.0001$ converge **48**, **603** and **205** iterations respectivelly. So it is possible to adjust those parameters in order to try to get better results, but we noticed that for classification purposes more iterations is better than a lower or higher learning rate $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overal conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
